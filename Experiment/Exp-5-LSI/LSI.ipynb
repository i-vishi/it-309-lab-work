{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "518oBPB-MSVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "09a94925-e6f1-4eca-af90-027027f6097c"
      },
      "source": [
        "\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim import corpora, similarities\n",
        "from gensim.models import LsiModel\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "\n",
        "#FUNCTIONS\n",
        "\n",
        "#function to filter out stopwords and apply word stemming\n",
        "def filter_words_and_get_word_stems(document, word_tokenizer, word_stemmer,\n",
        "                                    stopword_set, pattern_to_match_words=r\"[^\\w]\",\n",
        "                                    word_length_minimum_n_chars=2):\n",
        "    \"\"\"Remove multiple white spaces and all non word content from text and\n",
        "    extract words. Then filter out stopwords and words with a length smaller\n",
        "    than word_length_minimum and apply word stemmer to get wordstems. Finally\n",
        "    return word stems.\n",
        "    \"\"\"\n",
        "    document = re.sub(pattern_to_match_words, r\" \", document)\n",
        "    document = re.sub(r\"\\s+\", r\" \", document)\n",
        "    words = word_tokenizer.tokenize(document)\n",
        "    words_filtered = [word.lower()\n",
        "                      for word in words\n",
        "                      if word.lower() not in stopword_set and len(word) >= word_length_minimum_n_chars]\n",
        "    word_stems = [word_stemmer.lemmatize(word) for word in words_filtered]\n",
        "    return(word_stems)\n",
        "\n",
        "\n",
        "#INPUT\n",
        "\n",
        "#training text data to calculate TF-IDF model from\n",
        "documents_train = [\n",
        "  \"She goes to school.\",\n",
        "  \"She runs to the shop.\",\n",
        "  \"I go to school and he goes to the shop.\"]\n",
        "\n",
        "#test data text data to match \n",
        "document_test = \"He runs to school.\"\n",
        "\n",
        "\n",
        "#PREPROCESS\n",
        "\n",
        "#set stopword set, word stemmer and word tokenizer\n",
        "stopword_set = set(stopwords.words(\"english\"))\n",
        "word_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "word_stemmer =  nltk.WordNetLemmatizer()\n",
        "\n",
        "#apply cleaning, filtering and word stemming to training documents\n",
        "word_stem_arrays_train = [\n",
        "        filter_words_and_get_word_stems(\n",
        "                str(document),\n",
        "                word_tokenizer,\n",
        "                word_stemmer,\n",
        "                stopword_set\n",
        "                ) for document in documents_train]\n",
        "print(\"Word Stems of Training Documents:\", word_stem_arrays_train)\n",
        "\n",
        "#apply cleaning, filtering and word stemming to test document\n",
        "word_stem_array_test = filter_words_and_get_word_stems(\n",
        "        document_test,\n",
        "        word_tokenizer,\n",
        "        word_stemmer,\n",
        "        stopword_set)\n",
        "print(\"Word Stems of Test Document:\", word_stem_array_test)\n",
        "\n",
        "\n",
        "#PROCESS\n",
        "\n",
        "#create dictionary containing unique word stems of training documents\n",
        "#TF (term frequencies) or \"global weights\"\n",
        "dictionary = corpora.Dictionary(\n",
        "  word_stem_array_train\n",
        "  for word_stem_array_train in word_stem_arrays_train)\n",
        "print(\"Dictionary :\", dictionary)\n",
        "\n",
        "#create corpus containing word stem id from dictionary and word stem count\n",
        "#for each word in each document\n",
        "#DF (document frequencies, for all terms in each document) or \"local weights\"\n",
        "corpus = [\n",
        "  dictionary.doc2bow(word_stem_array_train)\n",
        "  for word_stem_array_train in word_stem_arrays_train]\n",
        "print(\"Corpus :\", corpus)\n",
        "\n",
        "#create LSI model (Latent Semantic Indexing) from corpus and dictionary\n",
        "#LSI model consists of Singular Value Decomposition (SVD) of\n",
        "#Term Document Matrix M: M = T x S x D'\n",
        "#and dimensionality reductions of T, S and D (\"Derivation\")\n",
        "lsi_model = LsiModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary #, num_topics = 2 #(opt. setting for explicit dim. change)\n",
        "        )\n",
        "print(\"Derivation of Term Matrix T of Training Document Word Stems: \",\n",
        "      lsi_model.get_topics())\n",
        "#Derivation of Term Document Matrix of Training Document Word Stems = M' x [Derivation of T]\n",
        "print(\"LSI Vectors of Training Document Word Stems: \",\n",
        "      [lsi_model[document_word_stems] for document_word_stems in corpus])\n",
        "\n",
        "#calculate cosine similarity matrix for all training document LSI vectors\n",
        "cosine_similarity_matrix = similarities.MatrixSimilarity(lsi_model[corpus])\n",
        "print(\"Cosine Similarities of LSI Vectors of Training Documents:\",\n",
        "      [row for row in cosine_similarity_matrix])\n",
        "\n",
        "#calculate LSI vector from word stem counts of the test document and the LSI model content\n",
        "vector_lsi_test = lsi_model[dictionary.doc2bow(word_stem_array_test)]\n",
        "print(\"LSI Vector Test Document:\", vector_lsi_test)\n",
        "\n",
        "#perform a similarity query against the corpus\n",
        "cosine_similarities_test = cosine_similarity_matrix[vector_lsi_test]\n",
        "print(\"Cosine Similarities of Test Document LSI Vectors to Training Documents LSI Vectors:\",\n",
        "      cosine_similarities_test)\n",
        "\n",
        "\n",
        "#OUTPUT\n",
        "\n",
        "#get text of test documents most similar training document\n",
        "most_similar_document_test = documents_train[np.argmax(cosine_similarities_test)]\n",
        "print(\"Most similar Training Document to Test Document:\", most_similar_document_test)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Word Stems of Training Documents: [['go', 'school'], ['run', 'shop'], ['go', 'school', 'go', 'shop']]\n",
            "Word Stems of Test Document: ['run', 'school']\n",
            "Dictionary : Dictionary(4 unique tokens: ['go', 'school', 'run', 'shop'])\n",
            "Corpus : [[(0, 1), (1, 1)], [(2, 1), (3, 1)], [(0, 2), (1, 1), (3, 1)]]\n",
            "Derivation of Term Matrix T of Training Document Word Stems:  [[ 0.79411857  0.47930412  0.05482989  0.36964434]\n",
            " [-0.2236068  -0.2236068   0.67082039  0.67082039]\n",
            " [ 0.26339267 -0.68576057 -0.54497127  0.40418197]]\n",
            "LSI Vectors of Training Document Word Stems:  [[(0, 1.2734226922603296), (1, -0.44721359549995787), (2, -0.4223679046003072)], [(0, 0.4244742307534433), (1, 1.3416407864998743), (2, -0.14078930153343533)], [(0, 2.4371856025006937), (2, 0.24520672699445195)]]\n",
            "Cosine Similarities of LSI Vectors of Training Documents: [array([ 1.0000000e+00, -1.9446199e-08,  8.6602545e-01], dtype=float32), array([-1.9446199e-08,  1.0000000e+00,  2.8867516e-01], dtype=float32), array([0.86602545, 0.28867516, 1.        ], dtype=float32)]\n",
            "LSI Vector Test Document: [(0, 0.5341340127734087), (1, 0.44721359549995854), (2, -1.2307318377285017)]\n",
            "Cosine Similarities of Test Document LSI Vectors to Training Documents LSI Vectors: [0.49999997 0.5        0.28867513]\n",
            "Most similar Training Document to Test Document: She runs to the shop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3EL1hA2Mj38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
